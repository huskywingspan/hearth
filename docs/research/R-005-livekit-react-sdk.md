# R-005: LiveKit React SDK — Connection Lifecycle & Audio Integration Guide

> **Status:** Complete  
> **Date:** 2026-02-10  
> **Priority:** High | **Blocks:** H-001, H-003  
> **Source:** LiveKit official docs (`docs.livekit.io/transport/`), `livekit/components-js` GitHub, `livekit/client-sdk-js` GitHub

---

## Summary

LiveKit's React SDK (`@livekit/components-react`) provides two API surfaces: a **stable Room-based API** (`LiveKitRoom` component) and a **newer Session-based API** (`SessionProvider` + `useSession`). For Hearth's spatial audio Portal, we need custom audio routing through Web Audio API — which means bypassing the standard `RoomAudioRenderer` and managing audio tracks manually. This document covers both API surfaces, track management, and the critical integration point for spatial audio processing.

---

## 1. Installation

```bash
npm install @livekit/components-react @livekit/components-styles livekit-client
```

| Package | Purpose | Size Impact |
|---------|---------|-------------|
| `livekit-client` | Core WebRTC room, track, participant classes | ~120KB gzipped |
| `@livekit/components-react` | React components + hooks | ~40KB gzipped |
| `@livekit/components-styles` | Default CSS for prefab components | ~15KB |

---

## 2. Two API Surfaces

### 2a. Session API (New — Beta)

The newer API uses `useSession` and `SessionProvider`. This wraps the Room lifecycle in a more opinionated way:

```tsx
import { SessionProvider, useSession, RoomAudioRenderer } from '@livekit/components-react';
import { TokenSource } from 'livekit-client';

const tokenSource = TokenSource.endpoint('/api/livekit-token');

function App() {
  const session = useSession(tokenSource, {
    roomName: 'my-portal',
    participantIdentity: 'alice',
    participantName: 'Alice',
  });

  useEffect(() => {
    session.start({ tracks: { microphone: { enabled: true } } });
    return () => { session.end(); };
  }, []);

  return (
    <SessionProvider session={session}>
      <RoomAudioRenderer />
      <ControlBar />
    </SessionProvider>
  );
}
```

**Token Source:** `TokenSource.endpoint(url)` makes a request to your backend to get a LiveKit JWT token. Hearth's PocketBase will have a custom endpoint that generates tokens.

### 2b. LiveKitRoom API (Stable)

The traditional API that directly accepts token and server URL:

```tsx
import { LiveKitRoom, RoomAudioRenderer } from '@livekit/components-react';

function App() {
  return (
    <LiveKitRoom
      serverUrl="wss://lk.hearth.example"
      token={liveKitToken}
      connect={true}
      audio={true}
      video={false}
      onConnected={() => console.log('Connected to Portal')}
      onDisconnected={(reason) => console.log('Disconnected:', reason)}
      onError={(error) => console.error('Room error:', error)}
    >
      <RoomAudioRenderer />
    </LiveKitRoom>
  );
}
```

### Recommendation for Hearth

**Use the Session API** (`useSession` + `SessionProvider`). It handles token refresh automatically via the `TokenSource` and is the direction LiveKit is moving. The `LiveKitRoom` component still works but the Session API is cleaner for Hearth's auth flow (token generated by PocketBase → passed via endpoint).

---

## 3. Key Hooks

### Participants

```typescript
import {
  useParticipants,          // All participants (local + remote)
  useRemoteParticipants,    // Remote only
  useLocalParticipant,      // Local participant state
  useIsSpeaking,            // Is a specific participant speaking?
  useSpeakingParticipants,  // All currently speaking participants
  useConnectionState,       // 'connected' | 'connecting' | 'disconnected' | 'reconnecting'
} from '@livekit/components-react';

// Example: Get all participants in the Portal
const participants = useParticipants();
const remoteParticipants = useRemoteParticipants();
const { localParticipant, isMicrophoneEnabled } = useLocalParticipant();
const connectionState = useConnectionState();
```

### Tracks

```typescript
import { useTracks } from '@livekit/components-react';
import { Track } from 'livekit-client';

// Get all microphone tracks (local + remote)
const audioTracks = useTracks([Track.Source.Microphone], {
  onlySubscribed: true,
});

// Each item is a TrackReference:
// { participant, publication, source, track }
audioTracks.forEach(trackRef => {
  console.log(trackRef.participant.identity, trackRef.track?.mediaStreamTrack);
});
```

### Active Speakers (via livekit-client events)

```typescript
import { RoomEvent } from 'livekit-client';

// Room-level event (gives all currently active speakers)
room.on(RoomEvent.ActiveSpeakersChanged, (speakers: Participant[]) => {
  // speakers = array of all participants currently speaking
});

// Per-participant
participant.on(ParticipantEvent.IsSpeakingChanged, (speaking: boolean) => {
  // speaking = true/false
  // participant.audioLevel = 0.0 to 1.0
});
```

---

## 4. Track Subscription & Management

### Default Behavior
By default, `autoSubscribe: true` — all published tracks are automatically subscribed to and rendered.

### Selective Subscription (Required for Spatial Audio)

For Hearth's Portal, we may want fine-grained control over audio subscriptions (e.g., only subscribe to participants within hearing range):

```typescript
import { Room, RoomEvent } from 'livekit-client';

const room = new Room({
  // ... room options
});

await room.connect(url, token, {
  autoSubscribe: false, // Manual subscription control
});

// Subscribe to specific participant's tracks
room.on(RoomEvent.TrackPublished, (publication, participant) => {
  if (isWithinHearingRange(participant)) {
    publication.setSubscribed(true);
  }
});

// Enable/disable without full unsubscribe (cheaper — no re-negotiation)
publication.setEnabled(false); // Stop receiving data
publication.setEnabled(true);  // Resume receiving data
```

**`subscribe` vs `enable`:**
- `setSubscribed(false)` → Full WebRTC renegotiation (expensive, use for out-of-range participants)
- `setEnabled(false)` → Pauses data flow without renegotiation (cheap, use for temporarily muted/distant participants)

### Volume Control

```typescript
// Set volume on a remote audio track (0.0 to 1.0)
track.setVolume(0.5);

// Set volume on a remote participant (affects microphone track by default)
remoteParticipant.setVolume(0.3);
```

---

## 5. Raw Audio Track Access (Critical for Spatial Audio)

This is the most important section for Hearth's Portal spatial audio. The `livekit-client` SDK exposes the raw `MediaStreamTrack` from each subscribed audio track, which can be routed through the Web Audio API.

### Approach A: `setWebAudioPlugins()` (Recommended)

`RemoteAudioTrack` has an **experimental** API to inject custom Web Audio nodes into its audio pipeline:

```typescript
import { RoomEvent, RemoteAudioTrack, RemoteTrackPublication } from 'livekit-client';

const audioContext = new AudioContext();

room.on(RoomEvent.TrackSubscribed, (track, publication, participant) => {
  if (track instanceof RemoteAudioTrack) {
    // Create a PannerNode for spatial positioning
    const panner = audioContext.createPanner();
    panner.distanceModel = 'linear';
    panner.panningModel = 'equalpower';
    panner.maxDistance = 1000;
    panner.refDistance = 1;
    panner.rolloffFactor = 1;

    // Set audio context on the track (required first)
    track.setAudioContext(audioContext);

    // Inject the PannerNode into the audio pipeline
    // SDK connects: source → [your nodes in order] → gain → destination
    track.setWebAudioPlugins([panner]);

    // Store reference for position updates
    participantPanners.set(participant.identity, panner);
  }
});

// Update panner position when participant moves on Portal canvas
function updateParticipantPosition(identity: string, x: number, y: number) {
  const panner = participantPanners.get(identity);
  if (panner) {
    panner.positionX.value = x;
    panner.positionY.value = y;
    panner.positionZ.value = 0; // 2D — flatten Z
  }
}
```

**How it works internally:**
```
element.srcObject (MediaStream)
  → context.createMediaStreamSource(srcObject)
    → [your plugin nodes, connected in order]
      → GainNode (SDK volume control)
        → context.destination
```

The SDK mutes the `<audio>` element (`element.volume = 0; element.muted = true`) and routes all audio through the Web Audio graph.

### Approach B: Manual MediaStreamTrack (Fallback)

If the experimental API proves unstable, access the raw `MediaStreamTrack` directly:

```typescript
room.on(RoomEvent.TrackSubscribed, (track, publication, participant) => {
  if (track.kind === Track.Kind.Audio) {
    // Get the raw MediaStreamTrack
    const mediaStreamTrack: MediaStreamTrack = track.mediaStreamTrack;

    // Create Web Audio pipeline manually
    const source = audioContext.createMediaStreamSource(
      new MediaStream([mediaStreamTrack])
    );
    const panner = audioContext.createPanner();
    // ... configure panner ...

    source.connect(panner);
    panner.connect(audioContext.destination);

    // Do NOT call track.attach() — we're handling audio ourselves
    // Do NOT use RoomAudioRenderer — it would double-play audio
  }
});
```

**Trade-off:** Approach B bypasses LiveKit's built-in volume control, audio output device selection, and Web Audio plugin system. Approach A integrates cleanly with the SDK.

### Approach Recommendation

**Start with Approach A** (`setWebAudioPlugins`). It's marked experimental but the code is clean and well-tested in the `livekit-client` source. Fall back to Approach B only if Approach A has issues with specific browsers.

---

## 6. `createAudioAnalyser` Utility

LiveKit provides a built-in audio analysis utility that shows the pattern for accessing track audio data:

```typescript
import { createAudioAnalyser } from 'livekit-client';

const { analyser, calculateVolume, cleanup } = createAudioAnalyser(track, {
  fftSize: 2048,
  smoothingTimeConstant: 0.8,
});

// Use for visual indicators (Ember glow intensity)
function updateEmberGlow() {
  const volume = calculateVolume(); // 0.0 to 1.0
  setEmberIntensity(volume);
  requestAnimationFrame(updateEmberGlow);
}

// Clean up when done
cleanup();
```

This creates an `AnalyserNode` connected to the track's `MediaStreamTrack`. Useful for the Ember (active speaker glow) visual.

---

## 7. Audio Rendering for Hearth

### What NOT to Use

**Do NOT use `RoomAudioRenderer`** for the Portal spatial voice experience. `RoomAudioRenderer` renders all remote audio tracks as invisible `<audio>` elements with uniform volume — no spatial positioning. It's a convenience component for standard video conferences.

### What TO Use

Build a custom audio renderer that:
1. Uses `useTracks([Track.Source.Microphone])` to get all audio track references
2. For each remote track, creates a `PannerNode` via `setWebAudioPlugins()`
3. Updates PannerNode positions based on the Portal canvas coordinates
4. Uses `createAudioAnalyser` for Ember glow visualization

### Custom Spatial Audio Renderer Pattern

```tsx
function PortalAudioRenderer() {
  const audioTracks = useTracks([Track.Source.Microphone], {
    onlySubscribed: true,
  });
  const audioContextRef = useRef<AudioContext | null>(null);
  const pannersRef = useRef<Map<string, PannerNode>>(new Map());

  // Initialize AudioContext on first user interaction
  const initAudio = useCallback(() => {
    if (!audioContextRef.current) {
      audioContextRef.current = new AudioContext();
    }
  }, []);

  // Set up spatial audio for each remote track
  useEffect(() => {
    const ctx = audioContextRef.current;
    if (!ctx) return;

    const remoteTracks = audioTracks.filter(t => !t.participant.isLocal);

    remoteTracks.forEach(trackRef => {
      const identity = trackRef.participant.identity;
      if (pannersRef.current.has(identity)) return; // Already set up

      const track = trackRef.track;
      if (track && 'setAudioContext' in track) {
        const panner = ctx.createPanner();
        panner.distanceModel = 'linear';
        panner.panningModel = 'equalpower';
        panner.refDistance = 50;
        panner.maxDistance = 500;
        panner.rolloffFactor = 1;

        (track as RemoteAudioTrack).setAudioContext(ctx);
        (track as RemoteAudioTrack).setWebAudioPlugins([panner]);
        pannersRef.current.set(identity, panner);
      }
    });

    // Cleanup removed tracks
    pannersRef.current.forEach((panner, identity) => {
      if (!remoteTracks.find(t => t.participant.identity === identity)) {
        panner.disconnect();
        pannersRef.current.delete(identity);
      }
    });
  }, [audioTracks]);

  return null; // No visible UI — audio only
}
```

---

## 8. Connection Events & Error Handling

```typescript
import { RoomEvent, ConnectionState, DisconnectReason } from 'livekit-client';

// Connection state changes
room.on(RoomEvent.ConnectionStateChanged, (state: ConnectionState) => {
  // 'connected' | 'connecting' | 'disconnected' | 'reconnecting'
});

// Disconnection with reason
room.on(RoomEvent.Disconnected, (reason?: DisconnectReason) => {
  // DisconnectReason.CLIENT_INITIATED | .DUPLICATE_IDENTITY |
  // .SERVER_SHUTDOWN | .PARTICIPANT_REMOVED | .ROOM_DELETED | etc.
});

// Media device errors
room.on(RoomEvent.MediaDevicesError, (error: Error) => {
  // Microphone/camera permission denied or device unavailable
});

// Track subscription failed
room.on(RoomEvent.TrackSubscriptionFailed, (trackSid: string) => {
  // Could not subscribe to a track
});
```

---

## 9. Token Generation (Server-Side)

LiveKit tokens must be generated server-side. For Hearth, this will be a PocketBase hook:

```go
// Go (PocketBase hook) — token generation endpoint
import "github.com/livekit/protocol/auth"

func generateLiveKitToken(identity, roomName string) (string, error) {
    at := auth.NewAccessToken(apiKey, apiSecret)
    grant := &auth.VideoGrant{
        RoomJoin: true,
        Room:     roomName,
    }
    at.SetVideoGrant(grant).
        SetIdentity(identity).
        SetName(identity).
        SetValidFor(time.Hour * 24)
    return at.ToJWT()
}
```

On the client, use `TokenSource.endpoint('/api/hearth/livekit-token')` to request tokens. The endpoint should validate the PocketBase auth token before issuing a LiveKit token.

---

## 10. API Quick Reference

### Components

| Component | Purpose | Use in Hearth? |
|-----------|---------|----------------|
| `SessionProvider` | Room context provider (new API) | Yes — wraps Portal |
| `LiveKitRoom` | Room context provider (stable API) | Alternative to SessionProvider |
| `RoomAudioRenderer` | Auto-renders all audio tracks | **NO** — bypasses spatial audio |
| `AudioTrack` | Renders a single audio track | Maybe — for non-spatial audio |
| `ControlBar` | Mic/camera/screenshare controls | Yes — mic toggle |
| `ConnectionState` | Shows connection status text | Yes — Portal status |
| `ParticipantLoop` | Iterates over participants | Yes — Portal avatars |

### Hooks

| Hook | Returns | Purpose |
|------|---------|---------|
| `useSession(tokenSource, opts)` | Session control object | Init/start/end room session |
| `useParticipants()` | `Participant[]` | All participants |
| `useRemoteParticipants()` | `RemoteParticipant[]` | Remote only |
| `useLocalParticipant()` | Local participant state | Mic status, track refs |
| `useTracks(sources)` | `TrackReference[]` | Track references by source |
| `useIsSpeaking(participant?)` | `boolean` | Is participant speaking |
| `useSpeakingParticipants()` | `Participant[]` | All currently speaking |
| `useConnectionState()` | `ConnectionState` | Room connection status |
| `useRoomContext()` | `Room` | Direct Room instance access |

### livekit-client Events

| Event | Fires When |
|-------|-----------|
| `RoomEvent.TrackSubscribed` | Remote track received |
| `RoomEvent.TrackUnsubscribed` | Remote track removed |
| `RoomEvent.ActiveSpeakersChanged` | Speaker list updated |
| `RoomEvent.ConnectionStateChanged` | Connection state transition |
| `RoomEvent.Disconnected` | Room disconnected |
| `RoomEvent.MediaDevicesError` | Mic/camera access failed |
| `RoomEvent.ParticipantConnected` | New participant joined |
| `RoomEvent.ParticipantDisconnected` | Participant left |

---

## 11. Gotchas

1. **`setWebAudioPlugins` is experimental** — marked `@internal @experimental` in source. API may change. But the implementation is solid and used internally by LiveKit for their noise filter. Low risk.

2. **AudioContext requires user interaction** — browsers block `AudioContext` creation before user gesture. Initialize it on first click/tap ("Enter Portal" button).

3. **`RoomAudioRenderer` vs custom audio** — Cannot use both. `RoomAudioRenderer` renders ALL remote audio tracks as `<audio>` elements. If you also route through Web Audio API, you'll get double audio.

4. **Token expiry** — LiveKit tokens expire. The `SessionProvider` + `TokenSource.endpoint()` pattern auto-refreshes. With `LiveKitRoom`, you need to manage token refresh manually.

5. **Muted tab audio** — Browsers may suspend `AudioContext` when the tab is not focused. Use `audioContext.resume()` on visibility change, and consider Hearth's philosophy: audio should persist even when the tab isn't focused (ambient presence).

6. **Selective subscription memory** — With `autoSubscribe: false`, you MUST manually subscribe to tracks. If you forget, users hear nothing. Implement a subscription manager that subscribes to all audio by default, then adjusts `setEnabled` based on distance.

---

## Notes for Builder

1. **Start with `SessionProvider` + `useSession`** for the room connection lifecycle.
2. **DO NOT use `RoomAudioRenderer`** in the Portal. Build the custom `PortalAudioRenderer` that routes through Web Audio PannerNodes.
3. **Use `setWebAudioPlugins([pannerNode])`** (Approach A) for spatial audio integration. This is the cleanest path.
4. **Use `createAudioAnalyser`** for the Ember glow effect (speaker volume visualization).
5. **Token generation** goes in a PocketBase hook — validates PB auth, then issues a LiveKit JWT via `livekit/protocol/auth`.
6. **For the Portal canvas:** Each remote participant gets a `PannerNode`. Their canvas position maps to `panner.positionX/Y`. The local user's position maps to `audioContext.listener.positionX/Y`. See R-006 for the spatial audio math.
